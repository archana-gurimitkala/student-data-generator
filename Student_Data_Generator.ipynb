{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Student Data Generator - Code for Colab\n"
   ],
   "metadata": {
    "id": "8M14XroSvqGz"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## Step 1: Install Packages\n",
    "!pip install -q gradio transformers torch huggingface_hub pandas openai"
   ],
   "metadata": {
    "id": "M-mTmXz9USNe"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Install/Update bitsandbytes for quantization (latest version)\n",
    "!pip install -U bitsandbytes accelerate"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "st2-qf2N2U-N",
    "outputId": "cb2866cb-e7a7-4ff1-f139-54599b68469b"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FW8nl3XRFrz0"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import gradio as gr\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Constants\n",
    "\n",
    "LLAMA = \"meta-llama/Llama-3.2-3B-Instruct\""
   ],
   "metadata": {
    "id": "q3D1_T0uG_Qh"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Sign in to HuggingFace Hub\n",
    "\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)\n",
    "\n"
   ],
   "metadata": {
    "id": "xYW8kQYtF-3L"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Option 2: Use OpenAI for Transcription"
   ],
   "metadata": {
    "id": "0ogUN9PIcKTd"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Sign in to OpenAI using Secrets in Colab\n",
    "\n",
    "\n",
    "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
    "openai = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Model for text generation (not audio)\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "\n",
    "print(\"\u2705 OpenAI setup complete\")"
   ],
   "metadata": {
    "id": "qP6OB2OeGC2C",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4713d713-cf03-41ed-92ac-8b9163ac39c2"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Generate Student Data Function\n",
    "\n",
    "def generate_student_data(description, num_students=10, use_openai=True):\n",
    "    \"\"\"\n",
    "    Generate synthetic student data based on description\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import pandas as pd\n",
    "\n",
    "    prompt = f\"\"\"Generate a dataset of {num_students} synthetic student records.\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "For each student, provide:\n",
    "- Full Name\n",
    "- Email (matching the name, format: firstname.lastname@university.edu)\n",
    "- Student ID (format: STU followed by 6 digits)\n",
    "- Age (between 18-25)\n",
    "- Major/Program\n",
    "- GPA (between 2.0-4.0, with 2 decimal places)\n",
    "- Year (Freshman, Sophomore, Junior, Senior)\n",
    "- Enrollment Date (YYYY-MM-DD format between 2020-2024)\n",
    "\n",
    "Return ONLY a JSON array, no other text. Format:\n",
    "[\n",
    "  {{\n",
    "    \"name\": \"John Doe\",\n",
    "    \"email\": \"john.doe@university.edu\",\n",
    "    \"student_id\": \"STU123456\",\n",
    "    \"age\": 20,\n",
    "    \"major\": \"Computer Science\",\n",
    "    \"gpa\": 3.75,\n",
    "    \"year\": \"Sophomore\",\n",
    "    \"enrollment_date\": \"2022-09-01\"\n",
    "  }}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        if use_openai and openai:\n",
    "            # Use OpenAI (faster)\n",
    "            response = openai.chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a data generator. Return only valid JSON array, no explanations or markdown.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.7\n",
    "            )\n",
    "            result = response.choices[0].message.content\n",
    "\n",
    "        else:\n",
    "            # Use Hugging Face (LLAMA model)\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                LLAMA,\n",
    "                device_map=\"auto\",\n",
    "                quantization_config=quantization_config\n",
    "            )\n",
    "\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a data generator. Return only valid JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "\n",
    "            inputs = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(model.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    inputs,\n",
    "                    max_new_tokens=2000,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True\n",
    "                )\n",
    "\n",
    "            result = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
    "\n",
    "        # Extract JSON from response\n",
    "        start_idx = result.find('[')\n",
    "        end_idx = result.rfind(']') + 1\n",
    "\n",
    "        if start_idx != -1 and end_idx > start_idx:\n",
    "            json_str = result[start_idx:end_idx]\n",
    "            data = json.loads(json_str)\n",
    "            df = pd.DataFrame(data)\n",
    "            return df\n",
    "        else:\n",
    "            return pd.DataFrame({\"Error\": [\"Could not parse JSON from response\"]})\n",
    "\n",
    "    except Exception as e:\n",
    "        return pd.DataFrame({\"Error\": [f\"Error generating data: {str(e)}\"]})\n"
   ],
   "metadata": {
    "id": "piEMmcSfMH-O"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ],
   "metadata": {
    "id": "UcRKUgcxMew6"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Gradio Interface for Student Data Generator\n",
    "\n",
    "def gradio_generate(description, num_students, use_openai_choice):\n",
    "    \"\"\"Wrapper function for Gradio\"\"\"\n",
    "    try:\n",
    "        num = int(num_students) if num_students else 10\n",
    "        use_ai = use_openai_choice if openai else False\n",
    "        df = generate_student_data(description, num_students=num, use_openai=use_ai)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        return pd.DataFrame({\"Error\": [str(e)]})\n",
    "\n",
    "# Create Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=gradio_generate,\n",
    "    inputs=[\n",
    "        gr.Textbox(\n",
    "            label=\"Describe the student data you want\",\n",
    "            placeholder=\"e.g., Computer Science students with high GPAs, Engineering majors from 2023, Business students with internships\",\n",
    "            lines=3\n",
    "        ),\n",
    "        gr.Number(\n",
    "            label=\"Number of students\",\n",
    "            value=10,\n",
    "            minimum=1,\n",
    "            maximum=50\n",
    "        ),\n",
    "        gr.Checkbox(\n",
    "            label=\"Use OpenAI (faster) - uncheck to use Hugging Face LLAMA model\",\n",
    "            value=True if openai else False\n",
    "        )\n",
    "    ],\n",
    "    outputs=gr.Dataframe(\n",
    "        label=\"Generated Student Data\",\n",
    "        interactive=True\n",
    "    ),\n",
    "    title=\"\ud83c\udf93 Synthetic Student Data Generator\",\n",
    "    description=\"Generate realistic student data for testing and development. Describe what kind of students you want!\",\n",
    "    examples=[\n",
    "        [\"Computer Science students with high GPAs\", 10, True],\n",
    "        [\"Business majors from 2022\", 15, True],\n",
    "        [\"Engineering students with internships\", 5, True],\n",
    "        [\"Freshman students enrolled in 2024\", 20, True]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch(share=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "id": "ca6GMuc61fvs",
    "outputId": "3d1a5033-a087-454b-c3a1-38d2521dfa3a"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "HdQnWEzW3lzP"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}