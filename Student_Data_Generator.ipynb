{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archana-gurimitkala/student-data-generator/blob/main/Student_Data_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Student Data Generator - Code for Colab\n"
      ],
      "metadata": {
        "id": "8M14XroSvqGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 1: Install Packages\n",
        "!pip install -q gradio transformers torch huggingface_hub pandas openai"
      ],
      "metadata": {
        "id": "M-mTmXz9USNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install/Update bitsandbytes for quantization (latest version)\n",
        "!pip install -U bitsandbytes accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "st2-qf2N2U-N",
        "outputId": "cb2866cb-e7a7-4ff1-f139-54599b68469b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.11.12)\n",
            "Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.49.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW8nl3XRFrz0"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import gradio as gr\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from openai import OpenAI\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "\n",
        "LLAMA = \"meta-llama/Llama-3.2-3B-Instruct\""
      ],
      "metadata": {
        "id": "q3D1_T0uG_Qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign in to HuggingFace Hub\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "xYW8kQYtF-3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 2: Use OpenAI for Transcription"
      ],
      "metadata": {
        "id": "0ogUN9PIcKTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign in to OpenAI using Secrets in Colab\n",
        "\n",
        "\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "openai = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "# Model for text generation (not audio)\n",
        "MODEL_NAME = \"gpt-4o-mini\"\n",
        "\n",
        "print(\"âœ… OpenAI setup complete\")"
      ],
      "metadata": {
        "id": "qP6OB2OeGC2C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4713d713-cf03-41ed-92ac-8b9163ac39c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… OpenAI setup complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Student Data Function\n",
        "\n",
        "def generate_student_data(description, num_students=10, use_openai=True):\n",
        "    \"\"\"\n",
        "    Generate synthetic student data based on description\n",
        "    \"\"\"\n",
        "    import json\n",
        "    import pandas as pd\n",
        "\n",
        "    prompt = f\"\"\"Generate a dataset of {num_students} synthetic student records.\n",
        "\n",
        "Description: {description}\n",
        "\n",
        "For each student, provide:\n",
        "- Full Name\n",
        "- Email (matching the name, format: firstname.lastname@university.edu)\n",
        "- Student ID (format: STU followed by 6 digits)\n",
        "- Age (between 18-25)\n",
        "- Major/Program\n",
        "- GPA (between 2.0-4.0, with 2 decimal places)\n",
        "- Year (Freshman, Sophomore, Junior, Senior)\n",
        "- Enrollment Date (YYYY-MM-DD format between 2020-2024)\n",
        "\n",
        "Return ONLY a JSON array, no other text. Format:\n",
        "[\n",
        "  {{\n",
        "    \"name\": \"John Doe\",\n",
        "    \"email\": \"john.doe@university.edu\",\n",
        "    \"student_id\": \"STU123456\",\n",
        "    \"age\": 20,\n",
        "    \"major\": \"Computer Science\",\n",
        "    \"gpa\": 3.75,\n",
        "    \"year\": \"Sophomore\",\n",
        "    \"enrollment_date\": \"2022-09-01\"\n",
        "  }}\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        if use_openai and openai:\n",
        "            # Use OpenAI (faster)\n",
        "            response = openai.chat.completions.create(\n",
        "                model=MODEL_NAME,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a data generator. Return only valid JSON array, no explanations or markdown.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=0.7\n",
        "            )\n",
        "            result = response.choices[0].message.content\n",
        "\n",
        "        else:\n",
        "            # Use Hugging Face (LLAMA model)\n",
        "            quantization_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "                bnb_4bit_quant_type=\"nf4\"\n",
        "            )\n",
        "\n",
        "            tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                LLAMA,\n",
        "                device_map=\"auto\",\n",
        "                quantization_config=quantization_config\n",
        "            )\n",
        "\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"You are a data generator. Return only valid JSON.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "\n",
        "            inputs = tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                add_generation_prompt=True,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(model.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    inputs,\n",
        "                    max_new_tokens=2000,\n",
        "                    temperature=0.7,\n",
        "                    do_sample=True\n",
        "                )\n",
        "\n",
        "            result = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
        "\n",
        "        # Extract JSON from response\n",
        "        start_idx = result.find('[')\n",
        "        end_idx = result.rfind(']') + 1\n",
        "\n",
        "        if start_idx != -1 and end_idx > start_idx:\n",
        "            json_str = result[start_idx:end_idx]\n",
        "            data = json.loads(json_str)\n",
        "            df = pd.DataFrame(data)\n",
        "            return df\n",
        "        else:\n",
        "            return pd.DataFrame({\"Error\": [\"Could not parse JSON from response\"]})\n",
        "\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame({\"Error\": [f\"Error generating data: {str(e)}\"]})\n"
      ],
      "metadata": {
        "id": "piEMmcSfMH-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "UcRKUgcxMew6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the function (optional - you can delete this if you want)\n",
        "import pandas as pd\n",
        "\n",
        "# Test with a simple example\n",
        "test_description = \"Computer Science students with high GPAs\"\n",
        "test_data = generate_student_data(test_description, num_students=5, use_openai=True)\n",
        "print(\"Test result:\")\n",
        "print(test_data)"
      ],
      "metadata": {
        "id": "6CujZRAgMimy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "431afb7a-996a-4db3-ce35-0ac94b8e2bf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test result:\n",
            "              name                           email student_id  age  \\\n",
            "0      Alice Smith      alice.smith@university.edu  STU123001   21   \n",
            "1  Michael Johnson  michael.johnson@university.edu  STU123002   19   \n",
            "2      Emily Davis      emily.davis@university.edu  STU123003   22   \n",
            "3      David Brown      david.brown@university.edu  STU123004   20   \n",
            "4    Sophia Wilson    sophia.wilson@university.edu  STU123005   23   \n",
            "\n",
            "              major   gpa       year enrollment_date  \n",
            "0  Computer Science  3.85     Junior      2021-09-01  \n",
            "1  Computer Science  3.92   Freshman      2023-01-15  \n",
            "2  Computer Science  3.78     Senior      2020-08-20  \n",
            "3  Computer Science  3.80  Sophomore      2022-09-01  \n",
            "4  Computer Science  3.95     Senior      2020-08-15  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio Interface for Student Data Generator\n",
        "\n",
        "def gradio_generate(description, num_students, use_openai_choice):\n",
        "    \"\"\"Wrapper function for Gradio\"\"\"\n",
        "    try:\n",
        "        num = int(num_students) if num_students else 10\n",
        "        use_ai = use_openai_choice if openai else False\n",
        "        df = generate_student_data(description, num_students=num, use_openai=use_ai)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame({\"Error\": [str(e)]})\n",
        "\n",
        "# Create Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=gradio_generate,\n",
        "    inputs=[\n",
        "        gr.Textbox(\n",
        "            label=\"Describe the student data you want\",\n",
        "            placeholder=\"e.g., Computer Science students with high GPAs, Engineering majors from 2023, Business students with internships\",\n",
        "            lines=3\n",
        "        ),\n",
        "        gr.Number(\n",
        "            label=\"Number of students\",\n",
        "            value=10,\n",
        "            minimum=1,\n",
        "            maximum=50\n",
        "        ),\n",
        "        gr.Checkbox(\n",
        "            label=\"Use OpenAI (faster) - uncheck to use Hugging Face LLAMA model\",\n",
        "            value=True if openai else False\n",
        "        )\n",
        "    ],\n",
        "    outputs=gr.Dataframe(\n",
        "        label=\"Generated Student Data\",\n",
        "        interactive=True\n",
        "    ),\n",
        "    title=\"ðŸŽ“ Synthetic Student Data Generator\",\n",
        "    description=\"Generate realistic student data for testing and development. Describe what kind of students you want!\",\n",
        "    examples=[\n",
        "        [\"Computer Science students with high GPAs\", 10, True],\n",
        "        [\"Business majors from 2022\", 15, True],\n",
        "        [\"Engineering students with internships\", 5, True],\n",
        "        [\"Freshman students enrolled in 2024\", 20, True]\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "ca6GMuc61fvs",
        "outputId": "3d1a5033-a087-454b-c3a1-38d2521dfa3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e1ce6e7783dde6a519.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e1ce6e7783dde6a519.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HdQnWEzW3lzP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}